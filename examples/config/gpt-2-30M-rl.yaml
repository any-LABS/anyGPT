model_config:
  name: 'gpt-2-30M'
  block_size: 64
  dropout: 0.2
  embedding_size: 384
  num_heads: 6
  num_layers: 6
  bias: false
  adapter_bottleneck_factor: 48
  fine_tune: true # training with fine-tuning enabled to add adapters since we'll skip the SFT stage

io_config:
  experiment_name: 'gpt-2'
  dataset: 'shakespeare_karpathy'
  log_every_n_steps: 1

training_config:
  learning_rate: 1.0e-3
  swa_lrs: 1.0e-4
  batch_size: 8
  accumulate_gradients: 8
  beta2: 0.99
  min_lr: 1.0e-4
  max_steps: 10000
  val_check_interval: 250
  limit_val_batches: 200
  seed: null

torch_config:
  compile: false

ppo_config:
  checkpoint: "results/gpt-2-pretrain/version_0/checkpoints/last.pt"
  shared_actor_critic: true
  beta: 0.1
  beta_kl: 0.2
  epochs: 500
  batch_size: 8
  buffer_size: 64
  observation_size: 8
  action_size: 32
  env_kwargs:
    label: "neutral"
    model_name: "j-hartmann/emotion-english-distilroberta-base"
    dataset: "shakespeare_karpathy"