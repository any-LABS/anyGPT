model_config:
  name: 'gpt-2-30M'
  block_size: 256
  dropout: 0.2
  embedding_size: 384
  num_heads: 6
  num_layers: 6
  bias: false

training_config:
  learning_rate: 0.5e-3
  swa_lrs: 0.5e-4
  batch_size: 32
  accumulate_gradients: 8
  beta2: 0.999
  min_lr: 0.5e-4
  max_steps: 5000
  grad_clip: 0.1
  val_check_interval: 250
  limit_val_batches: 200

io_config:
  experiment_name: 'gpt-2'
  dataset: 'shakespeare_karpathy'