{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"anyGPT","text":"<p>anyGPT is a general purpose library for training any type of GPT model. Support for gpt-1, gpt-2, and gpt-3 models. Inspired by nanoGPT by Andrej Karpathy, the goal of this project is to provide tools for the training and usage of GPT style large language models. The aim is to provide a tool that is</p> <ul> <li>production ready</li> <li>easily configurable</li> <li>scalable</li> <li>free and open-source</li> <li>accessible by general software engineers and enthusiasts</li> <li>easily reproducible and deployable</li> </ul> <p>You don't need a Ph.D. in Machine Learning or Natural Language Processing to use anyGPT.</p>"},{"location":"#installation","title":"Installation","text":"<p>Note: It is recommended that you set up of a python virtual environment using mamba, conda, or poetry. Eventually, I will have support for easy spin-up using docker.</p> <p>To install anyGPT:</p> <pre><code>pip install anyGPT\n</code></pre>"},{"location":"#dependencies","title":"Dependencies","text":"<ul> <li>torch &gt;= 2.0.0</li> <li>numpy</li> <li>transformers</li> <li>datasets</li> <li>tiktoken</li> <li>wandb</li> <li>tqdm</li> <li>PyYAML</li> <li>lightning</li> <li>tensorboard</li> </ul>"},{"location":"#features","title":"Features","text":""},{"location":"#current","title":"Current","text":"<ul> <li>CLI and config file driven GPT training</li> <li>Supports CPU, GPU, TPU, IPU, and HPU</li> <li>Distributed training strategies for training at scale</li> </ul>"},{"location":"#roadmap","title":"Roadmap","text":"<ul> <li>Documentation</li> <li>HuggingFace integration<ul> <li>Load pre-trained gpt models</li> <li>push to hub</li> </ul> </li> <li>Easy spin VM spinup/getting started with<ul> <li>Downloading of pre-trained models</li> <li>FastAPI end-points for containerized microservice deployment</li> <li>Gradio ChatGPT style interface for testing and experimentation</li> </ul> </li> <li>Fine-tuning of pre-trained models</li> <li>Reinforcement Learning from Human Feedback and Rules Base Reward Modeling for LLM alignment</li> <li>More dataformat support beyond hosted text files *</li> </ul>"},{"location":"#usage","title":"Usage","text":""},{"location":"#data-preparation","title":"Data Preparation","text":"<pre><code>anygpt-prepare-data -n shakespeare_complete -u https://www.gutenberg.org/cache/epub/100/pg100.txt\n</code></pre>"},{"location":"#training","title":"Training","text":"<p>Create a config file. In this example, I'll call it <code>gpt-2-30M.yaml</code>. You can also check out the examples.</p> <pre><code>model_config:\n  name: 'gpt-2-30M'\n  block_size: 256\n  dropout: 0.2\n  embedding_size: 384\n  num_heads: 6\n  num_layers: 6\n\ntraining_config:\n  learning_rate: 1.0e-3\n  batch_size: 8\n  accumulate_gradients: 8\n  beta2: 0.99\n  min_lr: 1.0e-4\n  max_steps: 5000\n  val_check_interval: 200\n  limit_val_batches: 100\n\nio_config:\n  experiment_name: 'gpt-2'\n  dataset: 'shakespeare_complete'\n</code></pre> <pre><code>anygpt-train gpt-2-30M.yaml\n</code></pre>"},{"location":"#inference","title":"Inference","text":"<pre><code>anygpt-run results/gpt-1/version_0/checkpoints/epoch=0-step=5000.ckpt \\\n\"JAQUES.\nAll the world\u2019s a stage,\nAnd all the men and women merely players;\nThey have their exits and their entrances,\nAnd one man in his time plays many parts,\"\n</code></pre>"},{"location":"#documentation","title":"Documentation","text":"<p>Coming soon!</p>"},{"location":"#limitations","title":"Limitations","text":"<p>TBD</p>"},{"location":"#license","title":"License","text":"<p>The goal of this project is to enable organizations, both large and small, to train and use GPT style Large Language Models. I believe the future is open-source, with people and organizations being able to train from scratch or fine-tune models and deploy to production without relying on gatekeepers. So I'm releasing this under an MIT license for the benefit of all and in the hope that the community will find it useful.</p>"}]}