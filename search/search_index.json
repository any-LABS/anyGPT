{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"anyGPT","text":"<p>anyGPT is a general purpose library for training any type of GPT model. Support for gpt-1, gpt-2, and gpt-3 models. Inspired by nanoGPT by Andrej Karpathy, the goal of this project is to provide tools for the training and usage of GPT style large language models. The aim is to provide a tool that is</p> <ul> <li>production ready</li> <li>easily configurable</li> <li>scalable</li> <li>free and open-source</li> <li>accessible by general software engineers and enthusiasts</li> <li>easily reproducible and deployable</li> </ul> <p>You don't need a Ph.D. in Machine Learning or Natural Language Processing to use anyGPT.</p>"},{"location":"#installation","title":"Installation","text":"<p>NOTE: It is recommended that you set up of a python virtual environment using mamba, conda, or poetry. To install anyGPT:</p> <pre><code>$ pip install anyGPT\n</code></pre>"},{"location":"#using-docker","title":"Using Docker","text":"<p>The Docker image supports GPU passthrough for training and inference. In order to enable GPU passthrough please follow the guide for installing the NVidia Container Toolkit for your OS.</p> <p>NOTE On Windows you need to follow the guide to get NVidia Container Toolkit setup on WSL2. Docker WSL2 Backend is required.</p> <p>Once NVidia Container Toolkit and Docker is setup correctly, build the Docker image</p> <pre><code>$ docker build -t anygpt .\n</code></pre> <p>Use the following command to login to the container interactively, and use anygpt as if it was on your local host</p> <pre><code>$ docker run --gpus all -it anygpt\n</code></pre>"},{"location":"#mounting-volumes","title":"Mounting Volumes","text":"<p>It is recommended to mount a local directory into your container in order to  share data between your local host and the container. This will allow you to  save trained checkpoints, reuse datasets between runs and more.</p> <pre><code>$ docker run --gpus all -v /path/to/local/dir:/data -it anygpt\n</code></pre> <p>The above example mounts <code>/path/to/local/dir</code> to the <code>/data</code> directory in the container, and all data and changes are shared between them dynamically.</p>"},{"location":"#non-interactive-docker","title":"Non interactive Docker","text":"<p>The above documentation explains how to run a Docker container with an interactive session of anyGPT. You can also run anyGPT commands to completion using Docker by overriding the entrypoint</p> <pre><code>$ docker run --gpus=all -v /path/to/your/data:/data --entrypoint anygpt-run -it anygpt /data/test.pt \"hello world\"\n</code></pre> <p>The above command runs <code>anygpt-run</code> with the parameters <code>/data/test.pt \"hello world\"</code></p>"},{"location":"#dependencies","title":"Dependencies","text":"<ul> <li>torch &gt;= 2.0.0</li> <li>numpy</li> <li>transformers</li> <li>datasets</li> <li>tiktoken</li> <li>wandb</li> <li>tqdm</li> <li>PyYAML</li> <li>lightning</li> <li>tensorboard</li> </ul>"},{"location":"#features","title":"Features","text":""},{"location":"#current","title":"Current","text":"<ul> <li>CLI and config file driven GPT training</li> <li>Supports CPU, GPU, TPU, IPU, and HPU</li> <li>Distributed training strategies for training at scale</li> <li>Easy spin up using Docker</li> <li>FastAPI end-points for containerized microservice deployment</li> <li>HuggingFace integration<ul> <li>Load pre-trained gpt models</li> </ul> </li> </ul>"},{"location":"#roadmap","title":"Roadmap","text":"<ul> <li>Documentation</li> <li>HuggingFace integration<ul> <li>push to hub</li> </ul> </li> <li>Easy spin VM spinup/getting started with<ul> <li>Downloading of pre-trained models</li> <li>Gradio ChatGPT style interface for testing and experimentation</li> </ul> </li> <li>Fine-tuning of pre-trained models</li> <li>Reinforcement Learning from Human Feedback and Rules Base Reward Modeling for LLM alignment</li> <li>More dataformat support beyond hosted text files *</li> </ul>"},{"location":"#usage","title":"Usage","text":""},{"location":"#data-preparation","title":"Data Preparation","text":"<pre><code>$ anygpt-prepare-data -n shakespeare_complete -u https://www.gutenberg.org/cache/epub/100/pg100.txt\n</code></pre>"},{"location":"#training","title":"Training","text":"<p>Create a config file. In this example, I'll call it <code>gpt-2-30M.yaml</code>. You can also check out the example configuration files.</p> gpt-2-30M.yaml<pre><code>model_config:\nname: 'gpt-2-30M'\nblock_size: 256\ndropout: 0.2\nembedding_size: 384\nnum_heads: 6\nnum_layers: 6\ntraining_config:\nlearning_rate: 1.0e-3\nbatch_size: 8\naccumulate_gradients: 8\nbeta2: 0.99\nmin_lr: 1.0e-4\nmax_steps: 5000\nval_check_interval: 200\nlimit_val_batches: 100\nio_config:\nexperiment_name: 'gpt-2'\ndataset: 'shakespeare_complete'\n</code></pre> <pre><code>$ anygpt-train gpt-2-30M.yaml\n</code></pre>"},{"location":"#inference","title":"Inference","text":"<pre><code>$ anygpt-run results/gpt-2-pretrain/version_0/checkpoints/last.pt \\\n\"JAQUES.\nAll the world\u2019s a stage,\nAnd all the men and women merely players;\nThey have their exits and their entrances,\nAnd one man in his time plays many parts,\"\n</code></pre>"},{"location":"#inference-microservice","title":"Inference Microservice","text":"<p>anyGPT supports running models as a hosted microservice with a singular endpoint for inference. To launch the microservice, use the <code>anygpt-serve</code> entrypoint.</p>"},{"location":"#commandline-options","title":"Commandline Options","text":"<pre><code>$ anygpt-serve -h\nusage: anyGPT inference service [-h] [--port PORT] [--log-level LOG_LEVEL] model\n\nLoads an anyGPT model and hosts it on a simple microservice that can run inference over the network.\n\npositional arguments:\n  model                 Path t0 the trained model checkpoint to load\n\noptions:\n  -h, --help            show this help message and exit\n--port PORT           Port to start the microservice on (default: 5000)\n--host HOST           Host to bind microservice to (default: 127.0.0.1)\n--log-level LOG_LEVEL\n                        uvicorn log level (default: info)\n</code></pre>"},{"location":"#example","title":"Example","text":"<pre><code>$ anygpt-serve results/gpt-2-pretrain/version_0/checkpoints/last.pt --port 5000 --log-level info\n</code></pre>"},{"location":"#sending-requests","title":"Sending Requests","text":"<p><code>anygpt-serve</code> uses FastAPI to serve the microservice. To see the available microservice api go to the  <code>/docs</code> endpoint in your browser once the microservice is started</p>"},{"location":"#spinning-up-microservice-in-docker","title":"Spinning up Microservice in Docker","text":"<pre><code>$ docker run --gpus=all -v /path/to/your/data:/data -p 5000:5000 --entrypoint anygpt-serve -it anygpt /data/test.pt --port 5000 --host 0.0.0.0 --log-level info\nINFO:     Started server process [1]\nINFO:     Waiting for application startup.\nINFO:     Application startup complete.\nINFO:     Uvicorn running on http://127.0.0.1:5000 (Press CTRL+C to quit)\n</code></pre>"},{"location":"#documentation","title":"Documentation","text":""},{"location":"#limitations","title":"Limitations","text":"<p>TBD</p>"},{"location":"#license","title":"License","text":"<p>The goal of this project is to enable organizations, both large and small, to train and use GPT style Large Language Models. I believe the future is open-source, with people and organizations being able to train from scratch or fine-tune models and deploy to production without relying on gatekeepers. So I'm releasing this under an MIT license for the benefit of all and in the hope that the community will find it useful.</p> <p>Released under MIT by @any-LABS.</p>"},{"location":"data-api-documentation/","title":"Table of Contents","text":"<ul> <li>anyGPT.data.experience_dataset</li> <li>anyGPT.data.next_token_dataset</li> <li>anyGPT.data.prepare_data</li> </ul>"},{"location":"data-api-documentation/#anygptdataexperience_dataset","title":"anyGPT.data.experience_dataset","text":""},{"location":"data-api-documentation/#anygptdatanext_token_dataset","title":"anyGPT.data.next_token_dataset","text":""},{"location":"data-api-documentation/#anygptdataprepare_data","title":"anyGPT.data.prepare_data","text":""},{"location":"data-prep/","title":"Data Preparation","text":"<pre><code>$ anygpt-prepare-data -n shakespeare_complete -u https://www.gutenberg.org/cache/epub/100/pg100.txt\n</code></pre>"},{"location":"environments-api-documentation/","title":"Table of Contents","text":"<ul> <li>anyGPT.environments.sequence_classification_env</li> <li>SequenceClassificationEnv<ul> <li>__init__</li> </ul> </li> </ul>"},{"location":"environments-api-documentation/#anygptenvironmentssequence_classification_env","title":"anyGPT.environments.sequence_classification_env","text":""},{"location":"environments-api-documentation/#sequenceclassificationenv-objects","title":"SequenceClassificationEnv Objects","text":"<pre><code>class SequenceClassificationEnv(gym.Env)\n</code></pre>"},{"location":"environments-api-documentation/#__init__","title":"__init__","text":"<pre><code>def __init__(\ndataset: str = None,\nrender_mode: str = None,\nblock_size: int = 2,\nlabel: str = \"clean\",\nmodel_name: str = \"madhurjindal/autonlp-Gibberish-Detector-492513457\",\nencoded: bool = True,\ndevice: str = \"cpu\")\n</code></pre> <p>Arguments:</p> <ul> <li><code>dataset</code>: The dataset to sample observations from.</li> <li><code>render_mode</code>:</li> <li><code>block_size</code>: The size of the observation string.</li> <li><code>label</code>: The label to optimize for. Will depend on the HF model loaded.</li> <li><code>model_name</code>: The name of the HF sequence classifier.</li> <li><code>encoded</code>: If set to true, observations and actions should be encoded with the proper gpt style tokenizer.</li> </ul>"},{"location":"inference-api-documentation/","title":"Table of Contents","text":"<ul> <li>anyGPT.inference.runner</li> </ul>"},{"location":"inference-api-documentation/#anygptinferencerunner","title":"anyGPT.inference.runner","text":""},{"location":"inference/","title":"Inference","text":"<pre><code>$ anygpt-run results/gpt-2-pretrain/version_0/checkpoints/last.pt \\\n\"JAQUES.\nAll the world\u2019s a stage,\nAnd all the men and women merely players;\nThey have their exits and their entrances,\nAnd one man in his time plays many parts,\"\n</code></pre>"},{"location":"installation/","title":"Installation","text":"<p>NOTE: It is recommended that you set up of a python virtual environment using mamba, conda, or poetry. To install anyGPT:</p> <pre><code>$ pip install anyGPT\n</code></pre>"},{"location":"installation/#using-docker","title":"Using Docker","text":"<p>The Docker image supports GPU passthrough for training and inference. In order to enable GPU passthrough please follow the guide for installing the NVidia Container Toolkit for your OS.</p> <p>NOTE On Windows you need to follow the guide to get NVidia Container Toolkit setup on WSL2. Docker WSL2 Backend is required.</p> <p>Once NVidia Container Toolkit and Docker is setup correctly, build the Docker image</p> <pre><code>$ docker build -t anygpt .\n</code></pre> <p>Use the following command to login to the container interactively, and use anygpt as if it was on your local host</p> <pre><code>$ docker run --gpus all -it anygpt\n</code></pre>"},{"location":"installation/#mounting-volumes","title":"Mounting Volumes","text":"<p>It is recommended to mount a local directory into your container in order to  share data between your local host and the container. This will allow you to  save trained checkpoints, reuse datasets between runs and more.</p> <pre><code>$ docker run --gpus all -v /path/to/local/dir:/data -it anygpt\n</code></pre> <p>The above example mounts <code>/path/to/local/dir</code> to the <code>/data</code> directory in the container, and all data and changes are shared between them dynamically.</p>"},{"location":"installation/#non-interactive-docker","title":"Non interactive Docker","text":"<p>The above documentation explains how to run a Docker container with an interactive session of anyGPT. You can also run anyGPT commands to completion using Docker by overriding the entrypoint</p> <pre><code>$ docker run --gpus=all -v /path/to/your/data:/data --entrypoint anygpt-run -it anygpt /data/test.ckpt \"hello world\"\n</code></pre> <p>The above command runs <code>anygpt-run</code> with the parameters <code>/data/test.ckpt \"hello world\"</code></p>"},{"location":"installation/#dependencies","title":"Dependencies","text":"<ul> <li>torch &gt;= 2.0.0</li> <li>numpy</li> <li>transformers</li> <li>datasets</li> <li>tiktoken</li> <li>wandb</li> <li>tqdm</li> <li>PyYAML</li> <li>lightning</li> <li>tensorboard</li> </ul>"},{"location":"microservice/","title":"Inference Microservice","text":"<p>anyGPT supports running models as a hosted microservice with a singular endpoint for inference. To launch the microservice, use the <code>anygpt-serve</code> entrypoint.</p>"},{"location":"microservice/#commandline-options","title":"Commandline Options","text":"<pre><code>$ anygpt-serve -h\nusage: anyGPT inference service [-h] [--port PORT] [--log-level LOG_LEVEL] model\n\nLoads an anyGPT model and hosts it on a simple microservice that can run inference over the network.\n\npositional arguments:\n  model                 Path t0 the trained model checkpoint to load\n\noptions:\n  -h, --help            show this help message and exit\n--port PORT           Port to start the microservice on (default: 5000)\n--host HOST           Host to bind microservice to (default: 127.0.0.1)\n--log-level LOG_LEVEL\n                        uvicorn log level (default: info)\n</code></pre>"},{"location":"microservice/#example","title":"Example","text":"<pre><code>$ anygpt-serve results/gpt-2-pretrain/version_0/checkpoints/last.pt --port 5000 --log-level info\n</code></pre>"},{"location":"microservice/#sending-requests","title":"Sending Requests","text":"<p><code>anygpt-serve</code> uses FastAPI to serve the microservice. To see the available microservice api go to the  <code>/docs</code> endpoint in your browser once the microservice is started</p>"},{"location":"microservice/#spinning-up-microservice-in-docker","title":"Spinning up Microservice in Docker","text":"<pre><code>$ docker run --gpus=all -v /path/to/your/data:/data -p 5000:5000 --entrypoint anygpt-serve -it anygpt /data/test.pt --port 5000 --host 0.0.0.0 --log-level info\nINFO:     Started server process [1]\nINFO:     Waiting for application startup.\nINFO:     Application startup complete.\nINFO:     Uvicorn running on http://127.0.0.1:5000 (Press CTRL+C to quit)\n</code></pre>"},{"location":"models-api-documentation/","title":"Table of Contents","text":"<ul> <li>anyGPT.models.anygpt</li> <li>anyGPT.models.anygpt_critic</li> <li>anyGPT.models.anygpt_lit</li> <li>anyGPT.models.anygpt_ppo_lit</li> <li>anyGPT.models.modules</li> <li>anyGPT.models.operators</li> <li>anyGPT.models.ppo_policy</li> </ul>"},{"location":"models-api-documentation/#anygptmodelsanygpt","title":"anyGPT.models.anygpt","text":""},{"location":"models-api-documentation/#anygptmodelsanygpt_critic","title":"anyGPT.models.anygpt_critic","text":""},{"location":"models-api-documentation/#anygptmodelsanygpt_lit","title":"anyGPT.models.anygpt_lit","text":""},{"location":"models-api-documentation/#anygptmodelsanygpt_ppo_lit","title":"anyGPT.models.anygpt_ppo_lit","text":""},{"location":"models-api-documentation/#anygptmodelsmodules","title":"anyGPT.models.modules","text":""},{"location":"models-api-documentation/#anygptmodelsoperators","title":"anyGPT.models.operators","text":""},{"location":"models-api-documentation/#anygptmodelsppo_policy","title":"anyGPT.models.ppo_policy","text":""},{"location":"settings-api-documentation/","title":"Table of Contents","text":"<ul> <li>anyGPT.config.settings</li> <li>SimpleConfig<ul> <li>__init__</li> <li>update</li> </ul> </li> <li>TorchConfig<ul> <li>backend</li> <li>device</li> <li>precision</li> <li>compile</li> <li>accelerator</li> </ul> </li> </ul>"},{"location":"settings-api-documentation/#anygptconfigsettings","title":"anyGPT.config.settings","text":""},{"location":"settings-api-documentation/#simpleconfig-objects","title":"SimpleConfig Objects","text":"<pre><code>class SimpleConfig()\n</code></pre> <p>SimpleConfig - base class for config dataclass objects</p> <p></p>"},{"location":"settings-api-documentation/#__init__","title":"__init__","text":"<pre><code>def __init__(**kwargs)\n</code></pre> <p>Initializes dataclass based on keys in the yaml dict.</p> <p>Arguments:</p> <ul> <li><code>kwargs</code>: a dictionary sourced from a yaml config file.</li> </ul> <p></p>"},{"location":"settings-api-documentation/#update","title":"update","text":"<pre><code>def update(kwargs)\n</code></pre> <p>Updates the values in the dataclass.</p> <p>Arguments:</p> <ul> <li><code>kwargs</code>: dictionary of key/value pairs to update.</li> </ul> <p></p>"},{"location":"settings-api-documentation/#torchconfig-objects","title":"TorchConfig Objects","text":"<pre><code>@dataclass\nclass TorchConfig(SimpleConfig)\n</code></pre> <p>Torch configuration.</p> <p></p>"},{"location":"settings-api-documentation/#backend","title":"backend","text":"<p>Specifies which backend to use. Default='nccl'. Currently disabled.</p> <p></p>"},{"location":"settings-api-documentation/#device","title":"device","text":"<p>Specifies which device to use. Default=cuda. Options are 'cpu', 'cuda'</p> <p></p>"},{"location":"settings-api-documentation/#precision","title":"precision","text":"<p>Specifies the precision used during training. Acceptable values are 32, 16-mixed, bf16-mixed, or 64.</p> <p></p>"},{"location":"settings-api-documentation/#compile","title":"compile","text":"<p>Specifies whether to compile the model for training. Default=true</p> <p></p>"},{"location":"settings-api-documentation/#accelerator","title":"accelerator","text":"<p>Specifies which accelerator to use. Default=auto.</p>"},{"location":"training-api-documentation/","title":"Table of Contents","text":"<ul> <li>anyGPT.training.model_checkpoint</li> <li>anyGPT.training.trainers</li> </ul>"},{"location":"training-api-documentation/#anygpttrainingmodel_checkpoint","title":"anyGPT.training.model_checkpoint","text":""},{"location":"training-api-documentation/#anygpttrainingtrainers","title":"anyGPT.training.trainers","text":""},{"location":"training/","title":"Training","text":"<p>Create a config file. In this example, I'll call it <code>gpt-2-30M.yaml</code>. You can also check out the [example configuration files][example-configs].</p> gpt-2-30M.yaml<pre><code>model_config:\nname: 'gpt-2-30M'\nblock_size: 256\ndropout: 0.2\nembedding_size: 384\nnum_heads: 6\nnum_layers: 6\ntraining_config:\nlearning_rate: 1.0e-3\nbatch_size: 8\naccumulate_gradients: 8\nbeta2: 0.99\nmin_lr: 1.0e-4\nmax_steps: 5000\nval_check_interval: 200\nlimit_val_batches: 100\nio_config:\nexperiment_name: 'gpt-2'\ndataset: 'shakespeare_complete'\n</code></pre> <pre><code>$ anygpt-train gpt-2-30M.yaml\n</code></pre>"}]}